{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSynthesizer.DataDescriber import DataDescriber\n",
    "from DataSynthesizer.DataGenerator import DataGenerator\n",
    "from DataSynthesizer.ModelInspector import ModelInspector\n",
    "from DataSynthesizer.lib.utils import read_json_file, display_bayesian_network\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-defined parameteres\n",
    "\n",
    "the input_data_files list of dictionaries consists of 4 keys.\n",
    "- input_file_path\n",
    "- candidate_keys\n",
    "- primary_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "   \n",
    "# Opening JSON file\n",
    "f = open('/data/datafiles/dataset_configuration.json',)\n",
    "\n",
    "input_data_files = json.load(f)\n",
    "\n",
    "f.close()\n",
    "# A filter of number of Rows, use this when you want to filter a number of rows \n",
    "# in a big dataset in order to use the Data Synthesizer Frontend. Use 0 if you want to retrieve all rows\n",
    "numberOfRowsFilter = 20000\n",
    "\n",
    "# location of two output files\n",
    "mode = 'independent_attribute_mode'\n",
    "destination_file_folder = f'/data/datafiles/out/{mode}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attribute is categorical if its domain size is less than this threshold.\n",
    "# Here modify the threshold to adapt to the domain size of \"education\" (which is 14 in input dataset).\n",
    "threshold_value = 20 \n",
    "\n",
    "\n",
    "# Number of tuples generated in synthetic dataset.\n",
    "num_tuples_to_generate = 20000 # Here 32561 is the same as input dataset, but it can be set to another number."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Data Synthesizer\n",
    "\n",
    "1. Data Synthesizer has some requirements for the dataset (for example, no boolean datatypes or columns which have 0 values)\n",
    "2. To accomodate to these requirements, the dataset will be prepared in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "filtered_output_files: list = []\n",
    "\n",
    "def prepare_dataframe_for_synthesizer(df: DataFrame, dropColumns: list, foreignKeys) -> DataFrame:\n",
    "    #replace boolean values by bit, because synthesizer does not accept boolean type\n",
    "    df =df.replace(True,1)\n",
    "    df = df.replace(False,0)\n",
    "    df = df.drop(columns=dropColumns) if len(dropColumns) > 0 else df\n",
    "    #drop columns which are empty\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_input_df(input_data_file) -> DataFrame:\n",
    "    inputDf: pd.DataFrame()\n",
    "    foreignKeys =input_data_file['foreign_keys']\n",
    "    if len(foreignKeys) == 0:\n",
    "        inputDf = pd.read_csv(filepath_or_buffer=input_data_file['input_file_path']\n",
    "                                                ,sep=\";\"\n",
    "                                                ,nrows=numberOfRowsFilter\n",
    "                                                ,low_memory=False) \n",
    "    if len(foreignKeys) > 0:\n",
    "        #only get related foreign key records when foreign keys are defined\n",
    "        for foreignKey in foreignKeys:\n",
    "            foreign_key_reference_file_path = f\"{os.path.dirname(foreignKey['reference_file'])}/{os.path.basename(foreignKey['reference_file']).split('.')[0]}_filtered.csv\"\n",
    "            foreign_key_df: pd.DataFrame = pd.read_csv(filepath_or_buffer=foreign_key_reference_file_path\n",
    "                                                ,sep=\",\"\n",
    "                                                ,low_memory=False)[foreignKey['reference_key']]\n",
    "\n",
    "            foreign_key_df_list = foreign_key_df.to_list()\n",
    "\n",
    "            for chunk in pd.read_csv(filepath_or_buffer=input_data_file['input_file_path']\n",
    "                                                ,sep=\";\"\n",
    "                                                , chunksize=10000):\n",
    "                if 'inputDf' in locals():           \n",
    "                    inputDf = pd.concat([inputDf,chunk[chunk[foreignKey['foreign_key']].isin(foreign_key_df_list)]])\n",
    "                else:\n",
    "                    inputDf = chunk[chunk[foreignKey['foreign_key']].isin(foreign_key_df_list)]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    return inputDf\n",
    "\n",
    "#filter input files based on numberOfRows Parameter\n",
    "for input_data_file in input_data_files:\n",
    "    filtered_output_data = {\"filtered_output_path\": f\"{os.path.dirname(input_data_file['input_file_path'])}/{os.path.basename(input_data_file['input_file_path']).split('.')[0]}_filtered.csv\"\n",
    "                                            ,\"candidate_keys\": input_data_file['candidate_keys']\n",
    "                                            ,\"description_file_path\": f\"{destination_file_folder}/{os.path.basename(input_data_file['input_file_path']).split('.')[0]}_description.json\"\n",
    "                                            ,\"synthetic_file_path\": f\"{destination_file_folder}/{os.path.basename(input_data_file['input_file_path']).split('.')[0]}_synthentic.csv\"\n",
    "                                            }\n",
    "    filtered_output_files.append(filtered_output_data)  \n",
    "    if numberOfRowsFilter > 0:\n",
    "        inputDf = get_input_df(input_data_file=input_data_file)                              \n",
    "        inputDf = prepare_dataframe_for_synthesizer(inputDf,dropColumns=input_data_file['drop_columns'], foreignKeys=input_data_file['foreign_keys'])\n",
    "        inputDf.to_csv(f\"{filtered_output_data['filtered_output_path']}\"\n",
    "                        ,sep=\",\"\n",
    "                        ,index=False\n",
    "                        ,quoting = csv.QUOTE_NONNUMERIC)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataDescriber\n",
    "\n",
    "1. Instantiate a DataDescriber.\n",
    "2. Compute the statistics of the dataset.\n",
    "3. Save dataset description to a file on local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "{'city': True, 'stars': True, 'review_count': True, 'is_open': True, 'categories0': True, 'categories1': True, 'categories2': True, 'categories3': True, 'categories4': True}\n",
      "/data/datafiles/yelp_academic_dataset_business_transformed_filtered.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_synthetic_data() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m generator: DataGenerator \u001b[39m=\u001b[39m DataGenerator()\n\u001b[1;32m     28\u001b[0m generator\u001b[39m.\u001b[39mgenerate_dataset_in_independent_mode(num_tuples_to_generate, filtered_data_file[\u001b[39m'\u001b[39m\u001b[39mdescription_file_path\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m generator\u001b[39m.\u001b[39;49msave_synthetic_data(filtered_data_file[\u001b[39m'\u001b[39;49m\u001b[39msynthetic_file_path\u001b[39;49m\u001b[39m'\u001b[39;49m],seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: save_synthetic_data() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "for filtered_data_file in filtered_output_files:\n",
    "        filteredDf: pd.DataFrame = pd.read_csv(filepath_or_buffer=filtered_data_file['filtered_output_path']\n",
    "                                                ,sep=\",\"\n",
    "                                                ,low_memory=False) \n",
    "        \n",
    "        columns: pd.Index = filteredDf.columns\n",
    "\n",
    "        categorical_attributes = {x:True for x in filteredDf.columns if x not in filtered_data_file['candidate_keys']}\n",
    "        candidate_keys = {x:True for x in filtered_data_file['candidate_keys']}\n",
    "        print(threshold_value)\n",
    "        describer: DataDescriber = DataDescriber(category_threshold=threshold_value)\n",
    "        print(categorical_attributes)\n",
    "        print(filtered_data_file['filtered_output_path'])\n",
    "        describer.describe_dataset_in_independent_attribute_mode(dataset_file=filtered_data_file['filtered_output_path'],\n",
    "                                                                epsilon=10,\n",
    "                                                                attribute_to_is_categorical=categorical_attributes,\n",
    "                                                                attribute_to_is_candidate_key=candidate_keys\n",
    "                                                                ,seed=0)\n",
    "        describer.save_dataset_description_to_file(filtered_data_file['description_file_path'])\n",
    "\n",
    "        ### Step 4 generate synthetic dataset\n",
    "\n",
    "        #1. Instantiate a DataGenerator.\n",
    "        #2. Generate a synthetic dataset.\n",
    "        #3. Save it to local machine.\n",
    "        generator: DataGenerator = DataGenerator()\n",
    "\n",
    "        generator.generate_dataset_in_independent_mode(num_tuples_to_generate, filtered_data_file['description_file_path'])\n",
    "        generator.save_synthetic_data(filtered_data_file['synthetic_file_path'],seed=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare the statistics of input and sythetic data (optional)\n",
    "\n",
    "The synthetic data is already saved in a file by step 4. The ModelInspector is for a quick test on the similarity between input and synthetic datasets.\n",
    "\n",
    "#### instantiate a ModelInspector.\n",
    "\n",
    "It needs input dataset, synthetic dataset, and attribute description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtered_data_file in filtered_output_files:\n",
    "        # Read both datasets using Pandas.\n",
    "        filteredDf: pd.DataFrame = pd.read_csv(filepath_or_buffer=filtered_data_file['filtered_output_path']\n",
    "                                                ,sep=\",\") \n",
    "        syntheticDf: pd.DataFrame = pd.read_csv(filepath_or_buffer=filtered_data_file['synthetic_file_path']\n",
    "                                                ,sep=\",\") \n",
    "        # Read attribute description from the dataset description file.\n",
    "        attribute_description = read_json_file(filtered_data_file['description_file_path'])['attribute_description']\n",
    "        inspector: ModelInspector = ModelInspector(filteredDf, syntheticDf, attribute_description)\n",
    "        inspector.mutual_information_heatmap()\n",
    "        #for attribute in syntheticDf.columns:\n",
    "        #        inspector.compare_histograms(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector.mutual_information_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, Dec  8 2022, 03:38:40) \n[GCC 8.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
